# Symbolic Knowledge Distillation: from General Language Models to Commonsense Models; West et al, 2022

## [Paper](https://aclanthology.org/2022.naacl-main.341/); Tags: #nlp-reading-group #symbolic-knowledge # common-sense

### (Abridged) Notes
Training a language model for common-sense knowledge, a common practice has humans author a knowledge graph—a new conceptual framework is proposed to leverage state-of-the-art models (i.e. GPT-3) for automatic knowledge graphs; the framework is known as knowledge distillation. In order to test symbolic distillation, comparisions are made with the **researchers previous work** on a human authored knowledge graph, ATOMIC<sup>20</sup><sub>20</sub> based on a relation-event and generated inference schemata. They feed the LLM and prompt GPT-3 to generate a conclusion based on the event; then they come up with a critic model (fine-tuned RoBERTa) and filter out dataset to ATOMIC<sup>10x</sup> resulting in the commonsense models, COMET<sup>DISTIL</sup>. 

> Knowledge Graphs being nodes with raw text inside and connected by relations that have a standardized form. The 7 commonsense types may not be a valid heuristic and crucially ignores quantifiers in its representation; increasing in ambiguity for X variables. Not touched upon

> Self-training is referenced as *from–machine–to–corpus–to–machine* in the paper; rewriting terminology and continuous reference to material throughout text (not a criticism, may be a good choice when petitioning to a reader but use wisely)

The idea of a student-teacher model is introduced and emphasized that the students performs drastically better on less data. Crucially, the paper was summarized during the NLP reading group as prompt GPT built with some annotation and a classifier to differentiate between correct and incorrect inferences resulting in improved performance on ATOMIC-related datasets... a good point was made that the paper was definitely successful in marketing this idea. ***Basically: What sounds cool?***

