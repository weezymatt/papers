# Reading Notes

This repository contains notes of papers I'm reading. (add slightly more detail)

Table of Contents
-----------------

* [Reading Notes](#reading-notes)
  * [Natural Language Processing](#nlp)
    * [Coreference Resolution](#coreference-resolution)
    * [Architectures](#architectures)
    * [Embeddings](#embeddings)
    * [N-Grams](#n-grams)
  * [Linguistics](#linguistics)
  * [Machine Learning](#machine-learning)
  * [Algorithms](#algorithms)


## NLP

### Coreference Resolution
1. Understanding Mention Detector-Linker Interaction in Neural Coreference Resolution; Wu and Gardner, 2021 [[Paper](https://aclanthology.org/2021.crac-1.16/)] [[Notes](https://github.com/weezymatt/papers/blob/main/2023/2021.crac-1.16.md)]
2. PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution; Chen et al, 2018 [[Paper](https://aclanthology.org/D18-1016/)] [[Notes](https://github.com/weezymatt/papers/blob/main/2023/D18-1016.md)]

###  Architectures 
1. Attention is all you need; Vaswani et al., 2023 [[Paper](https://arxiv.org/abs/1706.03762)] [[Notes](https://github.com/weezymatt/papers/blob/main/2023/1706.03762.md)]

### N-Grams
1. Show Some Love to Your *n*-grams: A Bit of Progress and Stronger *n*-gram Language Modeling Baselines; Shareghi et al., 2019 [[Paper](https://aclanthology.org/N19-1417/)] [[Notes](fillinlater)]

## Linguistics

## Machine Learning

## Algorithms
