# PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution; Chen et al, 2018

## [Paper](https://aclanthology.org/D18-1016/); Tags: #nlp #coreference-resolution

### Introduction
Coreference resolution is a _type_ of reference resolution that associates which DPs (also _mentions_) refer to the same object, concept, or entity in a document and the task of resolution has many applications (i.e., knowledge extraction, question answering, and dialog systems). The identification of mentions requires not only its lexicon but also contextual consideration, and needs the representation of entities before mention detection. Despite advanced techniques that represent distinct words as an array of numbers called a vector, and reproduce semantic and syntactic qualities—this remains a challenging task for resolution. Similar word embeddings consisting of proper names are hard to resolve and even identify.

### Related Work 
Numerous datasets have been proposed to study coreference resolution, such as MUC (Message Understanding Conference), ACE (Automatic Context Extraction), and OntoNotes (large-scale corpus) being widely used amongst researchers (Clark and Manning, 2016). **See more on Information Retrevial by Christopher Manning for further study.** However, novel datasets were proposed and OntoNotes was the de facto dataset to study coreference resolution.

### Dataset Creation
Preceeding work (Moosavi & Strube, 2017) suggests the overlap between training-test datasets marks a significant impact on resolution perfomance. OntoNotes consists of relatively low training-test overlap, and high overlap in machine learning allows for comparable statistics to evaluate performance (especially in models such as these) between datasets. Lastly, the research team highlighted the lack of singleton mentions as another limitation of the data, and in an effort to analyze the core challenges in reference resolution, the team proposed to build a new dataset PreoCo. The advantages of a PreCo enabled researchers to restrict the data domain to better study coreference resolution by easier error analysis. 

Several advantages of the proposal were highlighted: the corpus is 10 times larger than OntoNotes; the size of the vocabulary is appropriate for comprehension tasks; and the data is practical enough to collect. English reading comprehension tests for middle and high school Chinese students were used to simulate the vocabulary. PreCo contains open domain documents and has a unique writing style to enable better studying of coreference resolution. After the English tests are scraped from the web, information extraction is necessary to parse between various formats. Further data cleaning is then necessary with the help of rule-based approaches, such as regular expressions, and NLTK’s tokenizer. Next, mentions that corefer are manually annotated by part-time annotators to manually extract relevant content. Quality control is carefully practiced to preserve the integrity of the dataset6. **Because of limited resources on annotation, refinements are completed only for the development-test datasets. Limited annotation resources result in complete refinements only for the development and test sets.** Some datasets do not label singletons; singletons contribute to a large portion of the dataset and thus are costly to annotate.

### Analysis
The intention of creating PreCo is to simulate the core challenges involved in coreference resolution to better understand the bottlenecks of the system and understand the high and lower performance of the model. A strong baseline coreference model is used to evaluate how challenging the dataset is and study the impact of training-test overlap. The main evauluation metric for coreference is an avaeraged F1 score of the test set. PreCo performed 10 points higher than OntoNotes (EE2E-Coref: 81.5; OntoNotes: 70.4) and was likely attributed to the overlap between training and test sets. To study the training-test impact on coreference resolution, the research team calls an **error case** low-frequency words (LFW) if they are in its mention environment—**the lower the frequency rate is, the more clear the behavior of an algorithm may be**. The experiment confirms the relationship between the test set and coreference resolution by picking different subsets of the data and
evaluating the models.

### Conclusion
Coreference resolution systems must detect mentions and link them into clusters. Nowadays, the two tasks are performed jointly in an end-to-end model. In practice, mention detection emphasizes recall to avoid missing mentions that are untraceable in the linking process, suggesting a perfect recall is preferred. **With a perfect mention detection module, what is the performance on coreference resolution?**

While being a crucial task in the resolution process, this was not clearly quantified at the time, bearing into mind the design of existing datasets was not to improve resolution algorithms but serve as general purpose. Previous assumptions conclude a perfect mention detector can also make anaphoric decisions; linguistically speaking, this comes as a surprise since anaphora depends
upon expressions in context. Additionally, mention detectors use local information to derive their mentions and anaphoric decisions need more context, including long backtracking. In conclusion, this task should instead be completed in the clustering module.

The corresponding [[paper](https://aclanthology.org/2021.crac-1.16/)] introduces a neural end-to-end architecture using the PreCo and CoNLL-2012 dataset and dives deeper into understanding the impact of mention detection, mention clustering and how the task might be successfully separated into another task.
