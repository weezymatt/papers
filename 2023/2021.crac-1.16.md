# Understanding Mention Detector-Linker Interaction in Neural Coreference Resolution, Wu and Gardner, 2021

## [Paper](https://aclanthology.org/2021.crac-1.16/); Tags: #nlp #coreference-resolution

### Introduction
Coreference resolution (e.g. Jack and he in Jack remembered he had an appointment) is well known to be a difficult task in natural language processing, employing necessary scoring functions and token representation to avoid pronoun misclassification. Despite improved performance, few work examining the mention detection-linker interaction has been extended to current neural architecture. The best instantiations of this model, SpanBERT and c2f-coref, are used to investigate its two tasks: mention detection and mention linker.

### Methodology 
Modern systems for coreference resolution are based on supervised machine learning systems whose methods require annotated datasets; using the CoNLL-2012, the low-precision high-recall is the spotlight. Most design decisions emphasize recall for the detector, a large degradation is shown from noisy mentions and, increasing the number of mentions worsens performance; suggesting the importance of precision-recall balance. An analysis of the effects of precision-recall tradeoff is consistent and finds that fixing precision yields improvement, notably in anaphoric resolution. To quantify the effect of precision, the coreference score is examined for every span-antecedent pair. The re-trained model with perfect precision results in a higher average score (precision: -13.0; recall -15.1), indicating noise with perfect recall prevents reliable linking. On the other hand, fixing precision results in fewer mention errors but more missing errors. An imprecise detector has notable consequences on performance by complicating the linker’s learning burden.

### Anaphoricity Decisions
However, perfect mention precision requires the detachment of anaphoric and singleton mentions (i.e. mentions that not participate in coreference). Such a distinction is important, as these anaphoricity decisions account for most of performance improvement. To this end, detectors do not model anaphoric relationships. Two span classifiers are built to test the architecture’s anaphoric ability that recognize all mentions and anaphoric mentions. The investigation (mention f1: 79.89; anaphoricity f1: 54.32) support this claim that the former classifier achieves higher performance in detection but is unable to make anaphoric decisions. While the detector struggles handling anaphoricity, the linker explicitly displays this feature by using a dummy token to represent antecedents for non-anaphoric mentions. The experiment is extended to the linker, and results are in concord with similar studies (Lu & Ng 2020) where pronoun resolution contributes to a large source of error.

### Conclusion
Finally, the analysis of the complex interaction between mention detection and mention linking exposed higher anaphoric precision results in greater performance, though this task is difficult. In demonstrating perfect anaphoricity, a large source of errors are about pronoun resolution, and thus stipulate further coreference resolution research.
