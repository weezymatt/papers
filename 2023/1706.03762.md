# Attention is All You Need, Vaswani et al., 2023

## [Paper](https://arxiv.org/abs/1706.03762); Tags: #nlp #transformers

### Introduction
State of the art approaches in sequence modeling and transduction problems—learning to convert one string to another—are recurrent neural networks (RNN), long short-term memory (LSTM), and gated recurrent neural networks. Encoder-decoder architectures, found in recurrent or convolutional neural networks, can handle input and output sequences of variable length; hence RNN are typically suitable for seq-2-seq problems such as machine translation. The architecture first generates a sequence of hidden states *h<sub>t</sub>*

